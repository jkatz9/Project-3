{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "chunksize = 1000\n",
    "\n",
    "authors_df = pd.read_csv('data/authors.csv', chunksize=chunksize)\n",
    "cited_by_count_df = pd.read_csv('data/cited_by_count.csv', chunksize=chunksize)\n",
    "display_name_df = pd.read_csv('data/display_name.csv', chunksize=chunksize)\n",
    "predicted_gender_df = pd.read_csv(\n",
    "    'data/predicted_gender.csv', chunksize=chunksize)\n",
    "works_df = pd.read_csv('data/works.csv', chunksize=chunksize)\n",
    "works_api_url = pd.read_csv('data/works_api_url.csv', chunksize=chunksize)\n",
    "x_concepts_df = pd.read_csv('data/x_concepts.csv', chunksize=chunksize)\n",
    "year_df = pd.read_csv('data/year.csv', chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_url(chunk):\n",
    "    # Process the chunk here\n",
    "    # For example, you can print the first few rows of each chunk\n",
    "    chunk['id'] = chunk['id'].apply(lambda x: str(x).split('/')[-1])\n",
    "    # You can also perform other operations like filtering, aggregating, etc.\n",
    "    # For demonstration, let's just return the first few rows\n",
    "    return chunk\n",
    "\n",
    "\n",
    "def fix_author_id(chunk):\n",
    "    chunk.rename(columns={'id': 'author_id'}, inplace=True)\n",
    "    return chunk\n",
    "\n",
    "\n",
    "def fix_work_id(chunk):\n",
    "    chunk.rename(columns={'id': 'work_id'}, inplace=True)\n",
    "    return chunk\n",
    "# List of DataFrames (or iterators of chunks)\n",
    "\n",
    "\n",
    "dataframes = {\n",
    "    # 'authors.csv': authors_df,\n",
    "    # 'cited_by_count.csv': cited_by_count_df,\n",
    "    # 'display_name.csv': display_name_df,\n",
    "    # 'predicted_gender.csv': predicted_gender_df,\n",
    "    # 'works_api_url.csv': works_api_url,\n",
    "    # 'x_concepts.csv': x_concepts_df,\n",
    "    'year.csv': year_df\n",
    "}\n",
    "\n",
    "for file_name, df in dataframes.items():\n",
    "    # Initialize a flag to write headers\n",
    "    write_header = True\n",
    "    for chunk in df:\n",
    "        # Process each chunk\n",
    "        chunk = strip_url(chunk)\n",
    "        chunk = fix_author_id(chunk)\n",
    "\n",
    "        # Save the processed chunk to a new CSV file\n",
    "        chunk.to_csv(f'data/processed_{file_name}',\n",
    "                     mode='a', index=False, header=write_header)\n",
    "\n",
    "        # After the first chunk, set the flag to False to skip headers\n",
    "        write_header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['id', 'display_name', 'publication_date',\n",
    "           'open_access', 'authorships', 'cited_by_count', 'keywords']\n",
    "write_header = True\n",
    "for chunk in works_df:\n",
    "    chunk = chunk[columns]\n",
    "    # Process each chunk\n",
    "    chunk = strip_url(chunk)\n",
    "    chunk = fix_work_id(chunk)\n",
    "\n",
    "    # Save the processed chunk to a new CSV file\n",
    "    chunk.to_csv('data/processed_works.csv',\n",
    "                 mode='a', index=False, header=write_header)\n",
    "    write_header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 5948 rows\n",
      "Processing chunk with 5594 rows\n",
      "Processing chunk with 5601 rows\n",
      "Processing chunk with 6277 rows\n",
      "Processing chunk with 6028 rows\n",
      "Processing chunk with 6322 rows\n",
      "Processing chunk with 6489 rows\n",
      "Processing chunk with 5537 rows\n",
      "Processing chunk with 5244 rows\n",
      "Processing chunk with 6441 rows\n",
      "Processing chunk with 6117 rows\n",
      "Processing chunk with 6371 rows\n",
      "Processing chunk with 5811 rows\n",
      "Processing chunk with 5359 rows\n",
      "Processing chunk with 5780 rows\n",
      "Processing chunk with 5764 rows\n",
      "Processing chunk with 5330 rows\n",
      "Processing chunk with 5875 rows\n",
      "Processing chunk with 5861 rows\n",
      "Processing chunk with 7105 rows\n",
      "Processing chunk with 5008 rows\n",
      "Processing chunk with 7508 rows\n",
      "Processing chunk with 5819 rows\n",
      "Processing chunk with 6164 rows\n",
      "Processing chunk with 7166 rows\n",
      "Processing chunk with 5780 rows\n",
      "Processing chunk with 6023 rows\n",
      "Processing chunk with 5803 rows\n",
      "Processing chunk with 7417 rows\n",
      "Processing chunk with 6285 rows\n",
      "Processing chunk with 6592 rows\n",
      "Processing chunk with 6624 rows\n",
      "Processing chunk with 6623 rows\n",
      "Processing chunk with 8367 rows\n",
      "Processing chunk with 6259 rows\n",
      "Processing chunk with 5381 rows\n",
      "Processing chunk with 6527 rows\n",
      "Processing chunk with 5988 rows\n",
      "Processing chunk with 6876 rows\n",
      "Processing chunk with 5586 rows\n",
      "Processing chunk with 5367 rows\n",
      "Processing chunk with 7333 rows\n",
      "Processing chunk with 5808 rows\n",
      "Processing chunk with 6207 rows\n",
      "Processing chunk with 6572 rows\n",
      "Processing chunk with 5693 rows\n",
      "Processing chunk with 6442 rows\n",
      "Processing chunk with 5894 rows\n",
      "Processing chunk with 5830 rows\n",
      "Processing chunk with 5797 rows\n",
      "Processing chunk with 5492 rows\n",
      "Processing chunk with 5883 rows\n",
      "Processing chunk with 7036 rows\n",
      "Processing chunk with 5837 rows\n",
      "Processing chunk with 6891 rows\n",
      "Processing chunk with 7580 rows\n",
      "Processing chunk with 5805 rows\n",
      "Processing chunk with 5821 rows\n",
      "Processing chunk with 5075 rows\n",
      "Processing chunk with 6545 rows\n",
      "Processing chunk with 5660 rows\n",
      "Processing chunk with 4763 rows\n",
      "Processing chunk with 6351 rows\n",
      "Processing chunk with 4710 rows\n",
      "Processing chunk with 6958 rows\n",
      "Processing chunk with 5685 rows\n",
      "Processing chunk with 6845 rows\n",
      "Processing chunk with 6829 rows\n",
      "Processing chunk with 6497 rows\n",
      "Processing chunk with 8543 rows\n",
      "Processing chunk with 6313 rows\n",
      "Processing chunk with 5684 rows\n",
      "Processing chunk with 5734 rows\n",
      "Processing chunk with 5153 rows\n",
      "Processing chunk with 5939 rows\n",
      "Processing chunk with 6237 rows\n",
      "Processing chunk with 6717 rows\n",
      "Processing chunk with 6422 rows\n",
      "Processing chunk with 6086 rows\n",
      "Processing chunk with 5854 rows\n",
      "Processing chunk with 5452 rows\n",
      "Processing chunk with 5979 rows\n",
      "Processing chunk with 5342 rows\n",
      "Processing chunk with 5366 rows\n",
      "Processing chunk with 5980 rows\n",
      "Processing chunk with 6603 rows\n",
      "Processing chunk with 5591 rows\n",
      "Processing chunk with 5523 rows\n",
      "Processing chunk with 4733 rows\n",
      "Processing chunk with 7139 rows\n",
      "Processing chunk with 5405 rows\n",
      "Processing chunk with 7687 rows\n",
      "Processing chunk with 5751 rows\n",
      "Processing chunk with 5429 rows\n",
      "Processing chunk with 7406 rows\n",
      "Processing chunk with 6274 rows\n",
      "Processing chunk with 6600 rows\n",
      "Processing chunk with 6291 rows\n",
      "Processing chunk with 5797 rows\n",
      "Processing chunk with 5349 rows\n",
      "Processing chunk with 6083 rows\n",
      "Processing chunk with 9904 rows\n",
      "Processing chunk with 6347 rows\n",
      "Processing chunk with 7142 rows\n",
      "Processing chunk with 5564 rows\n",
      "Processing chunk with 5701 rows\n",
      "Processing chunk with 6309 rows\n",
      "Processing chunk with 6786 rows\n",
      "Processing chunk with 5753 rows\n",
      "Processing chunk with 6049 rows\n",
      "Processing chunk with 6145 rows\n",
      "Processing chunk with 6223 rows\n",
      "Processing chunk with 5567 rows\n",
      "Processing chunk with 7304 rows\n",
      "Processing chunk with 6553 rows\n",
      "Processing chunk with 6172 rows\n",
      "Processing chunk with 6125 rows\n",
      "Processing chunk with 7214 rows\n",
      "Processing chunk with 5993 rows\n",
      "Processing chunk with 6105 rows\n",
      "Processing chunk with 5661 rows\n",
      "Processing chunk with 5426 rows\n",
      "Processing chunk with 5050 rows\n",
      "Processing chunk with 6614 rows\n",
      "Processing chunk with 5599 rows\n",
      "Processing chunk with 5295 rows\n",
      "Processing chunk with 6520 rows\n",
      "Processing chunk with 7378 rows\n",
      "Processing chunk with 5738 rows\n",
      "Processing chunk with 6783 rows\n",
      "Processing chunk with 5608 rows\n",
      "Processing chunk with 6573 rows\n",
      "Processing chunk with 5806 rows\n",
      "Processing chunk with 5432 rows\n",
      "Processing chunk with 6748 rows\n",
      "Processing chunk with 7748 rows\n",
      "Processing chunk with 5721 rows\n",
      "Processing chunk with 5421 rows\n",
      "Processing chunk with 5924 rows\n",
      "Processing chunk with 5868 rows\n",
      "Processing chunk with 6511 rows\n",
      "Processing chunk with 7682 rows\n",
      "Processing chunk with 5544 rows\n",
      "Processing chunk with 5741 rows\n",
      "Processing chunk with 7634 rows\n",
      "Processing chunk with 6000 rows\n",
      "Processing chunk with 6195 rows\n",
      "Processing chunk with 8553 rows\n",
      "Processing chunk with 6247 rows\n",
      "Processing chunk with 5928 rows\n",
      "Processing chunk with 5942 rows\n",
      "Processing chunk with 5744 rows\n",
      "Processing chunk with 5542 rows\n",
      "Processing chunk with 5106 rows\n",
      "Processing chunk with 6594 rows\n",
      "Processing chunk with 6503 rows\n",
      "Processing chunk with 6417 rows\n",
      "Processing chunk with 8729 rows\n",
      "Processing chunk with 8995 rows\n",
      "Processing chunk with 5893 rows\n",
      "Processing chunk with 7316 rows\n",
      "Processing chunk with 5352 rows\n",
      "Processing chunk with 5961 rows\n",
      "Processing chunk with 6694 rows\n",
      "Processing chunk with 7635 rows\n",
      "Processing chunk with 5998 rows\n",
      "Processing chunk with 5136 rows\n",
      "Processing chunk with 5974 rows\n",
      "Processing chunk with 6327 rows\n",
      "Processing chunk with 6819 rows\n",
      "Processing chunk with 5867 rows\n",
      "Processing chunk with 7992 rows\n",
      "Processing chunk with 6528 rows\n",
      "Processing chunk with 5881 rows\n",
      "Processing chunk with 6150 rows\n",
      "Processing chunk with 5135 rows\n",
      "Processing chunk with 7143 rows\n",
      "Processing chunk with 6869 rows\n",
      "Processing chunk with 5788 rows\n",
      "Processing chunk with 6100 rows\n",
      "Processing chunk with 8431 rows\n",
      "Processing chunk with 5807 rows\n",
      "Processing chunk with 5118 rows\n",
      "Processing chunk with 6531 rows\n",
      "Processing chunk with 5928 rows\n",
      "Processing chunk with 6459 rows\n",
      "Processing chunk with 7044 rows\n",
      "Processing chunk with 6225 rows\n",
      "Processing chunk with 5708 rows\n",
      "Processing chunk with 5072 rows\n",
      "Processing chunk with 5626 rows\n",
      "Processing chunk with 5286 rows\n",
      "Processing chunk with 5218 rows\n",
      "Processing chunk with 5790 rows\n",
      "Processing chunk with 6650 rows\n",
      "Processing chunk with 8780 rows\n",
      "Processing chunk with 6019 rows\n",
      "Processing chunk with 5479 rows\n",
      "Processing chunk with 6228 rows\n",
      "Processing chunk with 6045 rows\n",
      "Processing chunk with 6249 rows\n",
      "Processing chunk with 6795 rows\n",
      "Processing chunk with 7703 rows\n",
      "Processing chunk with 7030 rows\n",
      "Processing chunk with 7490 rows\n",
      "Processing chunk with 6349 rows\n",
      "Processing chunk with 6438 rows\n",
      "Processing chunk with 5159 rows\n",
      "Processing chunk with 6284 rows\n",
      "Processing chunk with 6644 rows\n",
      "Processing chunk with 6347 rows\n",
      "Processing chunk with 6871 rows\n",
      "Processing chunk with 6067 rows\n",
      "Processing chunk with 6527 rows\n",
      "Processing chunk with 8151 rows\n",
      "Processing chunk with 5230 rows\n",
      "Processing chunk with 4968 rows\n",
      "Processing chunk with 5283 rows\n",
      "Processing chunk with 5860 rows\n",
      "Processing chunk with 5658 rows\n",
      "Processing chunk with 5905 rows\n",
      "Processing chunk with 5654 rows\n",
      "Processing chunk with 6930 rows\n",
      "Processing chunk with 6170 rows\n",
      "Processing chunk with 6450 rows\n",
      "Processing chunk with 6825 rows\n",
      "Processing chunk with 6118 rows\n",
      "Processing chunk with 6521 rows\n",
      "Processing chunk with 6540 rows\n",
      "Processing chunk with 5670 rows\n",
      "Processing chunk with 7543 rows\n",
      "Processing chunk with 6072 rows\n",
      "Processing chunk with 5476 rows\n",
      "Processing chunk with 5794 rows\n",
      "Processing chunk with 6295 rows\n",
      "Processing chunk with 6012 rows\n",
      "Processing chunk with 5792 rows\n",
      "Processing chunk with 5412 rows\n",
      "Processing chunk with 6089 rows\n",
      "Processing chunk with 7211 rows\n",
      "Processing chunk with 6016 rows\n",
      "Processing chunk with 7138 rows\n",
      "Processing chunk with 6398 rows\n",
      "Processing chunk with 7230 rows\n",
      "Processing chunk with 5936 rows\n",
      "Processing chunk with 6218 rows\n",
      "Processing chunk with 6384 rows\n",
      "Processing chunk with 6721 rows\n",
      "Processing chunk with 5541 rows\n",
      "Processing chunk with 7117 rows\n",
      "Processing chunk with 5345 rows\n",
      "Processing chunk with 6204 rows\n",
      "Processing chunk with 5367 rows\n",
      "Processing chunk with 5767 rows\n",
      "Processing chunk with 5789 rows\n",
      "Processing chunk with 6549 rows\n",
      "Processing chunk with 7022 rows\n",
      "Processing chunk with 5365 rows\n",
      "Processing chunk with 6181 rows\n",
      "Processing chunk with 5521 rows\n",
      "Processing chunk with 6144 rows\n",
      "Processing chunk with 5822 rows\n",
      "Processing chunk with 6791 rows\n",
      "Processing chunk with 6011 rows\n",
      "Processing chunk with 5831 rows\n",
      "Processing chunk with 5457 rows\n",
      "Processing chunk with 6306 rows\n",
      "Processing chunk with 6522 rows\n",
      "Processing chunk with 5799 rows\n",
      "Processing chunk with 7401 rows\n",
      "Processing chunk with 7898 rows\n",
      "Processing chunk with 5998 rows\n",
      "Processing chunk with 6083 rows\n",
      "Processing chunk with 7343 rows\n",
      "Processing chunk with 5559 rows\n",
      "Processing chunk with 8058 rows\n",
      "Processing chunk with 6225 rows\n",
      "Processing chunk with 6186 rows\n",
      "Processing chunk with 5862 rows\n",
      "Processing chunk with 7627 rows\n",
      "Processing chunk with 6777 rows\n",
      "Processing chunk with 6358 rows\n",
      "Processing chunk with 6914 rows\n",
      "Processing chunk with 6340 rows\n",
      "Processing chunk with 5495 rows\n",
      "Processing chunk with 7277 rows\n",
      "Processing chunk with 5584 rows\n",
      "Processing chunk with 6371 rows\n",
      "Processing chunk with 6896 rows\n",
      "Processing chunk with 6099 rows\n",
      "Processing chunk with 5473 rows\n",
      "Processing chunk with 5609 rows\n",
      "Processing chunk with 6306 rows\n",
      "Processing chunk with 6217 rows\n",
      "Processing chunk with 5957 rows\n",
      "Processing chunk with 8521 rows\n",
      "Processing chunk with 6258 rows\n",
      "Processing chunk with 5944 rows\n",
      "Processing chunk with 5925 rows\n",
      "Processing chunk with 6901 rows\n",
      "Processing chunk with 5912 rows\n",
      "Processing chunk with 5109 rows\n",
      "Processing chunk with 5130 rows\n",
      "Processing chunk with 4887 rows\n",
      "Processing chunk with 8390 rows\n",
      "Processing chunk with 6963 rows\n",
      "Processing chunk with 6133 rows\n",
      "Processing chunk with 5798 rows\n",
      "Processing chunk with 5612 rows\n",
      "Processing chunk with 6361 rows\n",
      "Processing chunk with 5158 rows\n",
      "Processing chunk with 6639 rows\n",
      "Processing chunk with 8205 rows\n",
      "Processing chunk with 6075 rows\n",
      "Processing chunk with 5203 rows\n",
      "Processing chunk with 5512 rows\n",
      "Processing chunk with 5151 rows\n",
      "Processing chunk with 6128 rows\n",
      "Processing chunk with 6550 rows\n",
      "Processing chunk with 5274 rows\n",
      "Processing chunk with 7839 rows\n",
      "Processing chunk with 7388 rows\n",
      "Processing chunk with 6780 rows\n",
      "Processing chunk with 6631 rows\n",
      "Processing chunk with 5570 rows\n",
      "Processing chunk with 5943 rows\n",
      "Processing chunk with 8631 rows\n",
      "Processing chunk with 5238 rows\n",
      "Processing chunk with 6316 rows\n",
      "Processing chunk with 6200 rows\n",
      "Processing chunk with 5977 rows\n",
      "Processing chunk with 8089 rows\n",
      "Processing chunk with 6846 rows\n",
      "Processing chunk with 7075 rows\n",
      "Processing chunk with 7148 rows\n",
      "Processing chunk with 5827 rows\n",
      "Processing chunk with 6777 rows\n",
      "Processing chunk with 6617 rows\n",
      "Processing chunk with 7428 rows\n",
      "Processing chunk with 5204 rows\n",
      "Processing chunk with 5175 rows\n",
      "Processing chunk with 5503 rows\n",
      "Processing chunk with 5782 rows\n",
      "Processing chunk with 6041 rows\n",
      "Processing chunk with 5321 rows\n",
      "Processing chunk with 6277 rows\n",
      "Processing chunk with 5320 rows\n",
      "Processing chunk with 5039 rows\n",
      "Processing chunk with 6091 rows\n",
      "Processing chunk with 6225 rows\n",
      "Processing chunk with 5713 rows\n",
      "Processing chunk with 6502 rows\n",
      "Processing chunk with 5875 rows\n",
      "Processing chunk with 5637 rows\n",
      "Processing chunk with 5903 rows\n",
      "Processing chunk with 7673 rows\n",
      "Processing chunk with 6406 rows\n",
      "Processing chunk with 6148 rows\n",
      "Processing chunk with 8116 rows\n",
      "Processing chunk with 5563 rows\n",
      "Processing chunk with 5623 rows\n",
      "Processing chunk with 4932 rows\n",
      "Processing chunk with 6721 rows\n",
      "Processing chunk with 5953 rows\n",
      "Processing chunk with 5088 rows\n",
      "Processing chunk with 7445 rows\n",
      "Processing chunk with 7199 rows\n",
      "Processing chunk with 5977 rows\n",
      "Processing chunk with 5958 rows\n",
      "Processing chunk with 7128 rows\n",
      "Processing chunk with 6643 rows\n",
      "Processing chunk with 6121 rows\n",
      "Processing chunk with 7394 rows\n",
      "Processing chunk with 5882 rows\n",
      "Processing chunk with 399 rows\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "works_df = pd.read_csv('data/works.csv', chunksize=chunksize)\n",
    "write_header = True\n",
    "for chunk in works_df:\n",
    "    chunk['authorships'] = chunk['authorships'].apply(ast.literal_eval)\n",
    "    au_chunk = chunk[['work_id', 'authorships']].explode('authorships')\n",
    "    au_chunk['author_id'] = au_chunk['authorships'].apply(\n",
    "        lambda x: x['author']['id'] if x['author'] is not None else None)\n",
    "    au_chunk['author_id'] = au_chunk['author_id'].apply(\n",
    "        lambda x: str(x).split('/')[-1] if x is not None else None)\n",
    "    au_chunk['author_display_name'] = au_chunk['authorships'].apply(\n",
    "        lambda x: x['author']['display_name'] if x['author'] is not None else None)\n",
    "    au_chunk.drop(columns=['authorships'], inplace=True)\n",
    "    print(f\"Processing chunk with {len(au_chunk)} rows\")\n",
    "    au_chunk.to_csv('data/processed_authorships.csv',\n",
    "                    mode='a', index=False, header=write_header)\n",
    "    write_header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 1000 rows\n",
      "Processing chunk with 81 rows\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "write_header = True\n",
    "for chunk in works_df:\n",
    "    chunk.drop(columns=['authorships'], inplace=True)\n",
    "    print(f\"Processing chunk with {len(chunk)} rows\")\n",
    "    chunk.to_csv('data/processed_works.csv',\n",
    "                 mode='a', index=False, header=write_header)\n",
    "    write_header = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "work_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "display_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "publication_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "open_access",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cited_by_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "keywords",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "81c05bd2-14aa-47b6-a47a-739697b1839c",
       "rows": [
        [
         "0",
         "W4382515348",
         "Penyuluhan dan Pelatihan Pembuatan Kompos Organik dengan Metode Keranjang Takakura di Desa Biring Ere",
         "2023-06-29",
         null,
         "1",
         "[]"
        ],
        [
         "1",
         "W4400300197",
         "Peningkatan Pengetahuan Ibu Melalui Penyuluhan Pentingnya Imunisasi Dasar Lengkap di Desa Biring Ere, Kecamatan, Bungoro, Kabupaten Pangkajene dan Kepulauan",
         "2023-11-01",
         null,
         "0",
         "[]"
        ],
        [
         "2",
         "W4302328439",
         "Polymorphism at codon 36 of the p53 gene.",
         "1994-01-01",
         null,
         "17",
         "[{'id': 'https://openalex.org/keywords/single-strand-conformation-polymorphism', 'display_name': 'Single-strand conformation polymorphism', 'score': 0.7228689}, {'id': 'https://openalex.org/keywords/coding-region', 'display_name': 'Coding region', 'score': 0.7117466}, {'id': 'https://openalex.org/keywords/genomic-dna', 'display_name': 'genomic DNA', 'score': 0.5521235}]"
        ],
        [
         "3",
         "W4391971651",
         "Facile synthesis of KOH and ball milling co-modified wheat straw-derived biochar for the efficient adsorption of methylene blue in aqueous solution",
         "2024-02-20",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/methylene-blue', 'display_name': 'Methylene blue', 'score': 0.846506}]"
        ],
        [
         "4",
         "W4403955429",
         "Facile Synthesis of Ball Milling and Koh Co-Modified Biochar from Wheat Straw for the Efficient Adsorption of Methylene Blue in Aqueous Solution: Preparation, Characterization, and Mechanism",
         "2024-01-01",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/methylene-blue', 'display_name': 'Methylene blue', 'score': 0.7509798}, {'id': 'https://openalex.org/keywords/rice-straw', 'display_name': 'Rice straw', 'score': 0.41044864}]"
        ],
        [
         "5",
         "W3049151413",
         "Réalisation d'un spectromètre de Fourier statique compact LLIFTS en optique intégrée sur verre",
         "2009-05-29",
         null,
         "0",
         "[]"
        ],
        [
         "6",
         "W2975492508",
         "Cabo Verde Egyptian Vulture<i>Neophron percnopterus</i>on the brink: community perceptions, inferences and facts of an extreme population crash",
         "2019-09-23",
         null,
         "2",
         "[{'id': 'https://openalex.org/keywords/vulture', 'display_name': 'Vulture', 'score': 0.65146744}, {'id': 'https://openalex.org/keywords/electrocution', 'display_name': 'Electrocution', 'score': 0.52259314}]"
        ],
        [
         "7",
         "W2915456352",
         "RESPOSTAS ECOLÓGICAS DAS POPULAÇÕES DE CAMARÕES PALAEMONIDAE A VARIAÇÃO DE FATORES ABIÓTICOS EM UM RESERVATÓRIO DO SEMIÁRIDO NORDESTINO",
         "2018-11-11",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/palaemonidae', 'display_name': 'Palaemonidae', 'score': 0.64295673}]"
        ],
        [
         "8",
         "W2114182016",
         "Gonadotrophins and ovarian steroids in cattle I. Pulsatile changes of concentrations in the jugular vein throughout the oestrous cycle",
         "1985-03-01",
         null,
         "67",
         "[{'id': 'https://openalex.org/keywords/luteolysis', 'display_name': 'Luteolysis', 'score': 0.74438536}, {'id': 'https://openalex.org/keywords/pulsatile-flow', 'display_name': 'Pulsatile flow', 'score': 0.64201915}, {'id': 'https://openalex.org/keywords/gonadotropin', 'display_name': 'Gonadotropin', 'score': 0.6031733}, {'id': 'https://openalex.org/keywords/jugular-vein', 'display_name': 'Jugular vein', 'score': 0.5675956}, {'id': 'https://openalex.org/keywords/basal', 'display_name': 'Basal (medicine)', 'score': 0.41846693}]"
        ],
        [
         "9",
         "W2947830255",
         "P070 The effect of distance from specialist health centre on disease severity in cystic fibrosis in the UK",
         "2019-05-27",
         null,
         "0",
         "[]"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>open_access</th>\n",
       "      <th>cited_by_count</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W4382515348</td>\n",
       "      <td>Penyuluhan dan Pelatihan Pembuatan Kompos Orga...</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W4400300197</td>\n",
       "      <td>Peningkatan Pengetahuan Ibu Melalui Penyuluhan...</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W4302328439</td>\n",
       "      <td>Polymorphism at codon 36 of the p53 gene.</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>[{'id': 'https://openalex.org/keywords/single-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W4391971651</td>\n",
       "      <td>Facile synthesis of KOH and ball milling co-mo...</td>\n",
       "      <td>2024-02-20</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 'https://openalex.org/keywords/methyle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W4403955429</td>\n",
       "      <td>Facile Synthesis of Ball Milling and Koh Co-Mo...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 'https://openalex.org/keywords/methyle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>W3049151413</td>\n",
       "      <td>Réalisation d'un spectromètre de Fourier stati...</td>\n",
       "      <td>2009-05-29</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>W2975492508</td>\n",
       "      <td>Cabo Verde Egyptian Vulture&lt;i&gt;Neophron percnop...</td>\n",
       "      <td>2019-09-23</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'id': 'https://openalex.org/keywords/vulture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>W2915456352</td>\n",
       "      <td>RESPOSTAS ECOLÓGICAS DAS POPULAÇÕES DE CAMARÕE...</td>\n",
       "      <td>2018-11-11</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 'https://openalex.org/keywords/palaemo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>W2114182016</td>\n",
       "      <td>Gonadotrophins and ovarian steroids in cattle ...</td>\n",
       "      <td>1985-03-01</td>\n",
       "      <td>None</td>\n",
       "      <td>67</td>\n",
       "      <td>[{'id': 'https://openalex.org/keywords/luteoly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>W2947830255</td>\n",
       "      <td>P070 The effect of distance from specialist he...</td>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       work_id                                       display_name  \\\n",
       "0  W4382515348  Penyuluhan dan Pelatihan Pembuatan Kompos Orga...   \n",
       "1  W4400300197  Peningkatan Pengetahuan Ibu Melalui Penyuluhan...   \n",
       "2  W4302328439          Polymorphism at codon 36 of the p53 gene.   \n",
       "3  W4391971651  Facile synthesis of KOH and ball milling co-mo...   \n",
       "4  W4403955429  Facile Synthesis of Ball Milling and Koh Co-Mo...   \n",
       "5  W3049151413  Réalisation d'un spectromètre de Fourier stati...   \n",
       "6  W2975492508  Cabo Verde Egyptian Vulture<i>Neophron percnop...   \n",
       "7  W2915456352  RESPOSTAS ECOLÓGICAS DAS POPULAÇÕES DE CAMARÕE...   \n",
       "8  W2114182016  Gonadotrophins and ovarian steroids in cattle ...   \n",
       "9  W2947830255  P070 The effect of distance from specialist he...   \n",
       "\n",
       "  publication_date open_access  cited_by_count  \\\n",
       "0       2023-06-29        None               1   \n",
       "1       2023-11-01        None               0   \n",
       "2       1994-01-01        None              17   \n",
       "3       2024-02-20        None               0   \n",
       "4       2024-01-01        None               0   \n",
       "5       2009-05-29        None               0   \n",
       "6       2019-09-23        None               2   \n",
       "7       2018-11-11        None               0   \n",
       "8       1985-03-01        None              67   \n",
       "9       2019-05-27        None               0   \n",
       "\n",
       "                                            keywords  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2  [{'id': 'https://openalex.org/keywords/single-...  \n",
       "3  [{'id': 'https://openalex.org/keywords/methyle...  \n",
       "4  [{'id': 'https://openalex.org/keywords/methyle...  \n",
       "5                                                 []  \n",
       "6  [{'id': 'https://openalex.org/keywords/vulture...  \n",
       "7  [{'id': 'https://openalex.org/keywords/palaemo...  \n",
       "8  [{'id': 'https://openalex.org/keywords/luteoly...  \n",
       "9                                                 []  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "write_header = True\n",
    "chunksize = 10000\n",
    "\n",
    "\n",
    "def parse_json(json_str):\n",
    "    try:\n",
    "        json_str = json_str.replace(\"'\", '\"')\n",
    "        # Handle single quotes in JSON string\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "\n",
    "works_df = pd.read_csv('data/works.csv', chunksize=chunksize,\n",
    "                       converters={'open_access': parse_json})\n",
    "\n",
    "\n",
    "# for chunk in works_df:\n",
    "#     chunk['open_access'] = chunk['open_access'].apply(\n",
    "#         lambda x: x.replace(\"'\", '\"'))\n",
    "#     chunk.dropna(subset=['open_access'], inplace=True)\n",
    "#     chunk['open_access'] = chunk['open_access'].apply(parse_json)\n",
    "#     # Process the chunk further if needed\n",
    "#     # For example, save the processed chunk to a new CSV file\n",
    "#     chunk.to_csv('data/processed_works_open_access.csv',\n",
    "#                  mode='a', index=False, header=write_header)\n",
    "#     write_header = False\n",
    "works_df.get_chunk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "work_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "display_name",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "publication_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "open_access",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cited_by_count",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "keywords",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "771b5582-66ee-4c99-bb57-f8ab958bbcad",
       "rows": [
        [
         "0",
         "W4382515348",
         "Penyuluhan dan Pelatihan Pembuatan Kompos Organik dengan Metode Keranjang Takakura di Desa Biring Ere",
         "2023-06-29",
         null,
         "1",
         "[]"
        ],
        [
         "1",
         "W4400300197",
         "Peningkatan Pengetahuan Ibu Melalui Penyuluhan Pentingnya Imunisasi Dasar Lengkap di Desa Biring Ere, Kecamatan, Bungoro, Kabupaten Pangkajene dan Kepulauan",
         "2023-11-01",
         null,
         "0",
         "[]"
        ],
        [
         "2",
         "W4302328439",
         "Polymorphism at codon 36 of the p53 gene.",
         "1994-01-01",
         null,
         "17",
         "[{'id': 'https://openalex.org/keywords/single-strand-conformation-polymorphism', 'display_name': 'Single-strand conformation polymorphism', 'score': 0.7228689}, {'id': 'https://openalex.org/keywords/coding-region', 'display_name': 'Coding region', 'score': 0.7117466}, {'id': 'https://openalex.org/keywords/genomic-dna', 'display_name': 'genomic DNA', 'score': 0.5521235}]"
        ],
        [
         "3",
         "W4391971651",
         "Facile synthesis of KOH and ball milling co-modified wheat straw-derived biochar for the efficient adsorption of methylene blue in aqueous solution",
         "2024-02-20",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/methylene-blue', 'display_name': 'Methylene blue', 'score': 0.846506}]"
        ],
        [
         "4",
         "W4403955429",
         "Facile Synthesis of Ball Milling and Koh Co-Modified Biochar from Wheat Straw for the Efficient Adsorption of Methylene Blue in Aqueous Solution: Preparation, Characterization, and Mechanism",
         "2024-01-01",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/methylene-blue', 'display_name': 'Methylene blue', 'score': 0.7509798}, {'id': 'https://openalex.org/keywords/rice-straw', 'display_name': 'Rice straw', 'score': 0.41044864}]"
        ],
        [
         "5",
         "W3049151413",
         "Réalisation d'un spectromètre de Fourier statique compact LLIFTS en optique intégrée sur verre",
         "2009-05-29",
         null,
         "0",
         "[]"
        ],
        [
         "6",
         "W2975492508",
         "Cabo Verde Egyptian Vulture<i>Neophron percnopterus</i>on the brink: community perceptions, inferences and facts of an extreme population crash",
         "2019-09-23",
         null,
         "2",
         "[{'id': 'https://openalex.org/keywords/vulture', 'display_name': 'Vulture', 'score': 0.65146744}, {'id': 'https://openalex.org/keywords/electrocution', 'display_name': 'Electrocution', 'score': 0.52259314}]"
        ],
        [
         "7",
         "W2915456352",
         "RESPOSTAS ECOLÓGICAS DAS POPULAÇÕES DE CAMARÕES PALAEMONIDAE A VARIAÇÃO DE FATORES ABIÓTICOS EM UM RESERVATÓRIO DO SEMIÁRIDO NORDESTINO",
         "2018-11-11",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/palaemonidae', 'display_name': 'Palaemonidae', 'score': 0.64295673}]"
        ],
        [
         "8",
         "W2114182016",
         "Gonadotrophins and ovarian steroids in cattle I. Pulsatile changes of concentrations in the jugular vein throughout the oestrous cycle",
         "1985-03-01",
         null,
         "67",
         "[{'id': 'https://openalex.org/keywords/luteolysis', 'display_name': 'Luteolysis', 'score': 0.74438536}, {'id': 'https://openalex.org/keywords/pulsatile-flow', 'display_name': 'Pulsatile flow', 'score': 0.64201915}, {'id': 'https://openalex.org/keywords/gonadotropin', 'display_name': 'Gonadotropin', 'score': 0.6031733}, {'id': 'https://openalex.org/keywords/jugular-vein', 'display_name': 'Jugular vein', 'score': 0.5675956}, {'id': 'https://openalex.org/keywords/basal', 'display_name': 'Basal (medicine)', 'score': 0.41846693}]"
        ],
        [
         "9",
         "W2947830255",
         "P070 The effect of distance from specialist health centre on disease severity in cystic fibrosis in the UK",
         "2019-05-27",
         null,
         "0",
         "[]"
        ],
        [
         "10",
         "W3169816407",
         "P080 Temporal disparity between hospital episode statistics and the UK cystic fibrosis Registry",
         "2021-01-01",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/patient-registry', 'display_name': 'Patient registry', 'score': 0.4759174}]"
        ],
        [
         "11",
         "W3149505076",
         "第12章 学会, 普及会, 日本照明委員会の活動",
         "2000-08-01",
         null,
         "0",
         "[]"
        ],
        [
         "12",
         "W3125107394",
         "乳酸菌スターター添加による「きりざい」の保存と食味に関する研究",
         "2013-01-01",
         null,
         "0",
         "[]"
        ],
        [
         "13",
         "W2034505997",
         "Head Injury: Developing Community Occupational Therapy to Meet the Challenge",
         "1992-03-01",
         null,
         "3",
         "[]"
        ],
        [
         "14",
         "W4396241768",
         "Urban waterlogging risk susceptibility within changing pattern of rainfall intensity in Delhi, India",
         "2024-04-29",
         null,
         "4",
         "[{'id': 'https://openalex.org/keywords/waterlogging', 'display_name': 'Waterlogging (archaeology)', 'score': 0.8652104}, {'id': 'https://openalex.org/keywords/intensity', 'display_name': 'Intensity', 'score': 0.5325792}, {'id': 'https://openalex.org/keywords/new-delhi', 'display_name': 'New delhi', 'score': 0.48715255}]"
        ],
        [
         "15",
         "W2302983178",
         "L’exportation du logiciel bulldozair pour le suivi numerique de chantier sur le marche hispanophone",
         "2015-11-12",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/exportation', 'display_name': 'Exportation', 'score': 0.5738493}]"
        ],
        [
         "16",
         "W4401689375",
         "Realizations and future projects of the high contrast imaging balloon system (HiCIBaS) platform",
         "2024-08-19",
         null,
         "0",
         "[]"
        ],
        [
         "17",
         "W4390729547",
         "Light emissions of ratiometric aliphatic redox polymer from canonical, anion, and anion-aggregate: Reduction-associated naked eye detections of Hg(II), Fe(III), and Cu(II)",
         "2024-01-10",
         null,
         "6",
         "[{'id': 'https://openalex.org/keywords/naked-eye', 'display_name': 'Naked eye', 'score': 0.5271081}]"
        ],
        [
         "18",
         "W4394013621",
         "Excited-State Intramolecular Proton- and Inter-Polymer Charge-Transfer of Semiconducting Redox Polymers for Fe(III), Cd(II), and Hg(II) Sensing",
         "2024-04-06",
         null,
         "5",
         "[{'id': 'https://openalex.org/keywords/hydroxymethyl', 'display_name': 'Hydroxymethyl', 'score': 0.5608888}, {'id': 'https://openalex.org/keywords/amide', 'display_name': 'Amide', 'score': 0.5336462}]"
        ],
        [
         "19",
         "W4406364306",
         "Exploring structures-properties and interactions of acrylic-hydrogel adsorbents with metal ions and organics using nuclear magnetic resonance and Fourier transform infrared spectroscopies",
         "2025-01-01",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/acrylic-acid', 'display_name': 'Acrylic acid', 'score': 0.51562613}]"
        ],
        [
         "20",
         "W4280598531",
         "Ratiometric pH Sensing, Photophysics, and Cell Imaging of Nonaromatic Light-Emitting Polymers",
         "2022-05-17",
         null,
         "22",
         "[{'id': 'https://openalex.org/keywords/proton-nmr', 'display_name': 'Proton NMR', 'score': 0.4288794}]"
        ],
        [
         "21",
         "W4399887309",
         "Synthesis of Intrinsically‐Fluorescent Aliphatic Tautomeric Polymers for Proton‐Conductivity, Dual‐State Emission, and Sensing/Oxidation‐Reduction of Metal Ions",
         "2024-06-21",
         null,
         "3",
         "[{'id': 'https://openalex.org/keywords/oxidation-state', 'display_name': 'Oxidation state', 'score': 0.44155613}]"
        ],
        [
         "22",
         "W4284887891",
         "Nontraditional Redox Active Aliphatic Luminescent Polymer for Ratiometric pH Sensing and Sensing‐Removal‐Reduction of Cu(II): Strategic Optimization of Composition",
         "2022-07-07",
         null,
         "15",
         "[{'id': 'https://openalex.org/keywords/stokes-shift', 'display_name': 'Stokes shift', 'score': 0.72211653}]"
        ],
        [
         "23",
         "W4400589532",
         "Synthesis of Semisynthetic Dual-State Fluorescent Polymers and Roles of Starch/Pectin on AIET-Assisted Dual-Emission, Conductivity, and Metal-Ion Sensing",
         "2024-07-12",
         null,
         "2",
         "[{'id': 'https://openalex.org/keywords/pectin', 'display_name': 'Pectin', 'score': 0.6408995}]"
        ],
        [
         "24",
         "W4402943193",
         "Synthesis and optimization of chitosan-incorporated semisynthetic polymer/α-Fe2O3 nanoparticle hybrid polymer to explore optimal efficacy of fluorescence resonance energy transfer/charge transfer for Co(II) and Ni(II) sensing",
         "2024-09-01",
         null,
         "2",
         "[]"
        ],
        [
         "25",
         "W4403249445",
         "Exploring Through-Space Charge Transfer-Mediated Optoelectrochemical Properties of Dual-State Luminescent Aliphatic Polymers and Optoelectronic Responses toward Metal Ions",
         "2024-10-09",
         null,
         "2",
         "[{'id': 'https://openalex.org/keywords/dual-role', 'display_name': 'Dual role', 'score': 0.44877306}]"
        ],
        [
         "26",
         "W4376620235",
         "Solid waste collagen-associated fabrication of magnetic hematite nanoparticle@collagen nanobiocomposite for emission-adsorption of dyes",
         "2023-05-15",
         null,
         "7",
         "[]"
        ],
        [
         "27",
         "W4403789809",
         "Uncovering Integrated Dual‐State ESIPT‐Conductivity, Redox‐Capacity, and Opto‐Electronic Responses Toward Hg(II)/ Cr(III) of Aliphatic Fluorescent Polymers",
         "2024-10-26",
         null,
         "1",
         "[]"
        ],
        [
         "28",
         "W69027329",
         "説明的文章指導における「筆者」概念に関する一考察",
         "2014-03-20",
         null,
         "0",
         "[]"
        ],
        [
         "29",
         "W4232945296",
         "Erratum: Pavages des simplexes, schémas de graphes recollés et compactification des PGL r n+1 /PGL r",
         "2001-09-01",
         null,
         "1",
         "[]"
        ],
        [
         "30",
         "W2609358829",
         "Le fibré équivariant universel sur la variété torique des facettes des pavages",
         "2003-04-29",
         null,
         "0",
         "[]"
        ],
        [
         "31",
         "W2794114856",
         "Tegillarca granosa extract Haishengsu inhibits tumor activity via a mitochondrial‑mediated apoptotic pathway",
         "2018-03-01",
         null,
         "4",
         "[]"
        ],
        [
         "32",
         "W1171206974",
         "Uwagi co do charakteru terminu określonego w art. 27 ust. 2 ustawy z 16 lutego 2007 r. o zapasach ropy naftowej, produktów naftowych i gazu ziemnego oraz zasadach postępowania w sytuacjach zagrożenia bezpieczeństwa paliwowego państwa i zakłóceń na rynku naftowym",
         "2013-01-01",
         null,
         "30",
         "[]"
        ],
        [
         "33",
         "W1012960291",
         "Obowiązek informacyjny wytwórców energii w przypadku obniżenia zapasów paliw - uwagi praktyczne",
         "2012-01-01",
         null,
         "0",
         "[]"
        ],
        [
         "34",
         "W2967409527",
         "Iconography : Tumeurs hépatocytaires bénignes : formes atypiques et pièges diagnostiques",
         "2013-07-20",
         null,
         "0",
         "[]"
        ],
        [
         "35",
         "W4315781349",
         "Inventaris van het archief van de diocesane, later Limburgse commissie van samenwerking",
         "2023-01-12",
         null,
         "0",
         "[]"
        ],
        [
         "36",
         "W4315786716",
         "De aardewerkfabriek van Frederik Regout, 1891-1896",
         "2023-01-12",
         null,
         "0",
         "[]"
        ],
        [
         "37",
         "W4315786881",
         "Inventaris van het archief van Henri de Beaumont's metaalindustrie N.V. en voorgangers, gevestigd te Maastricht",
         "2023-01-12",
         null,
         "0",
         "[]"
        ],
        [
         "38",
         "W2253913302",
         "我國“三農”問題的特徵分析及政策選擇",
         "2004-07-20",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/industrialisation', 'display_name': 'Industrialisation', 'score': 0.82453966}, {'id': 'https://openalex.org/keywords/consumption', 'display_name': 'Consumption', 'score': 0.5347829}, {'id': 'https://openalex.org/keywords/rural-settlement', 'display_name': 'Rural settlement', 'score': 0.503864}]"
        ],
        [
         "39",
         "W3120224400",
         "مطالعه فرکانس ارتعاش سازه پل به کمک دستگاه ارتعاش-سنج لیزری دوپلری",
         "2020-08-22",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/direct-conversion-receiver', 'display_name': 'Direct-conversion receiver', 'score': 0.49077043}]"
        ],
        [
         "40",
         "W3031352980",
         "辅助性T细胞17与调节性T细胞在肠道免疫研究的进展",
         "2013-04-08",
         null,
         "0",
         "[]"
        ],
        [
         "41",
         "W3137297175",
         "OBITUARIO… CONVERSACIÓN EN TORNO A JOSÉ MARÍA MANJAVACAS RUIZ. IN MEMORIAM.",
         "2021-01-01",
         null,
         "0",
         "[]"
        ],
        [
         "42",
         "W1921662574",
         "Mens en organisatie in het bedrijf. Een evaluatie van nieuwere literatuur",
         "1962-01-01",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/fundamental-human-needs', 'display_name': 'Fundamental human needs', 'score': 0.63791764}, {'id': 'https://openalex.org/keywords/special-needs', 'display_name': 'Special Needs', 'score': 0.46059832}, {'id': 'https://openalex.org/keywords/need-theory', 'display_name': 'Need theory', 'score': 0.4567136}]"
        ],
        [
         "43",
         "W4402949646",
         "Financial Signaling and Stock Return Movements: New Evidences in Indonesian Stock Markets after Covid-19",
         "2024-09-28",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/stock', 'display_name': 'Stock (firearms)', 'score': 0.59471446}, {'id': 'https://openalex.org/keywords/2019-20-coronavirus-outbreak', 'display_name': '2019-20 coronavirus outbreak', 'score': 0.45341334}]"
        ],
        [
         "44",
         "W4289357199",
         "The Role of Learning Organization in Developing Competency of Micro, Small, and Medium Enterprises (MSME) in Indonesia During Covid-19 Pandemic",
         "2022-01-01",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/pandemic', 'display_name': 'Pandemic', 'score': 0.7057666}]"
        ],
        [
         "45",
         "W4379529719",
         "ANALYSIS OF STOCK PRICE MOVEMENTS ON THE JAKARTA ISLAMIC INDEX DURING THE COVID-19 PANDEMIC",
         "2021-12-28",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/pandemic', 'display_name': 'Pandemic', 'score': 0.6445284}, {'id': 'https://openalex.org/keywords/stock', 'display_name': 'Stock (firearms)', 'score': 0.5600193}]"
        ],
        [
         "46",
         "W2809699449",
         "La interdisciplinariedad, necesidad para una educación ambiental de calidad",
         "2012-09-18",
         null,
         "0",
         "[]"
        ],
        [
         "47",
         "W4383604516",
         "The interdisciplinary method, a need for quality environmental education",
         "2012-09-01",
         null,
         "0",
         "[{'id': 'https://openalex.org/keywords/environmental-quality', 'display_name': 'Environmental Quality', 'score': 0.44303393}]"
        ],
        [
         "48",
         "W3214811776",
         "Claim study of contractors in Commercial Berth Project in Duqm Port in Oman",
         "2021-08-30",
         null,
         "1",
         "[{'id': 'https://openalex.org/keywords/port', 'display_name': 'Port (circuit theory)', 'score': 0.5997993}, {'id': 'https://openalex.org/keywords/scope', 'display_name': 'Scope (computer science)', 'score': 0.5761459}, {'id': 'https://openalex.org/keywords/statement-of-work', 'display_name': 'Statement of work', 'score': 0.4516307}]"
        ],
        [
         "49",
         "W2744135462",
         "Manual administrativo- contable y financiero para el gobierno autónomo descentralizado de la parroquia Miguel Egas Cabezas del cantón Otavalo, provincia de Imbabura",
         "2016-09-23",
         null,
         "0",
         "[]"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10000
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>open_access</th>\n",
       "      <th>cited_by_count</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W4382515348</td>\n",
       "      <td>Penyuluhan dan Pelatihan Pembuatan Kompos Orga...</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W4400300197</td>\n",
       "      <td>Peningkatan Pengetahuan Ibu Melalui Penyuluhan...</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W4302328439</td>\n",
       "      <td>Polymorphism at codon 36 of the p53 gene.</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>[{'id': 'https://openalex.org/keywords/single-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W4391971651</td>\n",
       "      <td>Facile synthesis of KOH and ball milling co-mo...</td>\n",
       "      <td>2024-02-20</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 'https://openalex.org/keywords/methyle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W4403955429</td>\n",
       "      <td>Facile Synthesis of Ball Milling and Koh Co-Mo...</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 'https://openalex.org/keywords/methyle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>W4401890391</td>\n",
       "      <td>Student Interpersonal Intelligence In The Age ...</td>\n",
       "      <td>2024-08-25</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 'https://openalex.org/keywords/publish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>W4399811878</td>\n",
       "      <td>DAMPAK EKOWISATA KAWAH PUTIH DI KECAMATAN RANC...</td>\n",
       "      <td>2024-06-19</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>W4400267619</td>\n",
       "      <td>Laminated insulated glass units under blast lo...</td>\n",
       "      <td>2024-07-03</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 'https://openalex.org/keywords/laminat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>W2079892328</td>\n",
       "      <td>Control and synchronizing nonlinear systems wi...</td>\n",
       "      <td>2013-09-04</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'id': 'https://openalex.org/keywords/synchro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>W2373773494</td>\n",
       "      <td>Simulation analysis on collision strengths of ...</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 'https://openalex.org/keywords/rollove...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          work_id                                       display_name  \\\n",
       "0     W4382515348  Penyuluhan dan Pelatihan Pembuatan Kompos Orga...   \n",
       "1     W4400300197  Peningkatan Pengetahuan Ibu Melalui Penyuluhan...   \n",
       "2     W4302328439          Polymorphism at codon 36 of the p53 gene.   \n",
       "3     W4391971651  Facile synthesis of KOH and ball milling co-mo...   \n",
       "4     W4403955429  Facile Synthesis of Ball Milling and Koh Co-Mo...   \n",
       "...           ...                                                ...   \n",
       "9995  W4401890391  Student Interpersonal Intelligence In The Age ...   \n",
       "9996  W4399811878  DAMPAK EKOWISATA KAWAH PUTIH DI KECAMATAN RANC...   \n",
       "9997  W4400267619  Laminated insulated glass units under blast lo...   \n",
       "9998  W2079892328  Control and synchronizing nonlinear systems wi...   \n",
       "9999  W2373773494  Simulation analysis on collision strengths of ...   \n",
       "\n",
       "     publication_date open_access cited_by_count  \\\n",
       "0          2023-06-29        None              1   \n",
       "1          2023-11-01        None              0   \n",
       "2          1994-01-01        None             17   \n",
       "3          2024-02-20        None              0   \n",
       "4          2024-01-01        None              0   \n",
       "...               ...         ...            ...   \n",
       "9995       2024-08-25        None              0   \n",
       "9996       2024-06-19        None              0   \n",
       "9997       2024-07-03        None              0   \n",
       "9998       2013-09-04        None              2   \n",
       "9999       2010-01-01        None              0   \n",
       "\n",
       "                                               keywords  \n",
       "0                                                    []  \n",
       "1                                                    []  \n",
       "2     [{'id': 'https://openalex.org/keywords/single-...  \n",
       "3     [{'id': 'https://openalex.org/keywords/methyle...  \n",
       "4     [{'id': 'https://openalex.org/keywords/methyle...  \n",
       "...                                                 ...  \n",
       "9995  [{'id': 'https://openalex.org/keywords/publish...  \n",
       "9996                                                 []  \n",
       "9997  [{'id': 'https://openalex.org/keywords/laminat...  \n",
       "9998  [{'id': 'https://openalex.org/keywords/synchro...  \n",
       "9999  [{'id': 'https://openalex.org/keywords/rollove...  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
